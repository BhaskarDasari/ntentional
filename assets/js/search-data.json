{
  
    
        "post0": {
            "title": "ICLR 2020: Efficient NLP - Transformers",
            "content": "I was lucky enough to volunteer and attend (virtual) ICLR 2020. It delivered a huge amount of learning for me and I was fortunate to join some really great discussions. . Efficient NLP was big focus of many of the papers and here I will focus on a few of the more well known transformer architectures proposed over the past year or so; Reformer, ELECTRA, Lite Transformer and ALBERT. Towards the end of this article I also mention additional ICLR summaries that are worth reading 🙂 . Note: ICLR Videos Now Online! . All of the ICLR paper talks and slides are now online, I highly recommend watching the 5 to 15minutes videos accompanying each of the papers below for some excellent summaries and additional understanding #ICLR2020 Public Archive - https://t.co/EpXWIK0ujS * ~700 short talks with synced slides, papers, and code* 8 keynotes with moderated QA * 15 workshops on topics ranging from climate change to AfricaNLP. pic.twitter.com/FVX2JJUYVZ . &mdash; Sasha Rush (@srush_nlp) May 4, 2020 . Efficient NLP - Transformers . New transformer architectures that promise less compute-intense NLP training, in order of my excitement to use them: . &#9889; Reformer: The Efficient Transformer &#9889; . Reformer enables training on much longer sequences than BERT-like models (e.g. document-length sequences instead of 512 token length sequences) much more efficiently | Reformer introduces a couple of techniques that improve both time and memory efficiency: . | Technique 1: Reversible residual connection layers (originally used in computer vision in RevNets) instead of the standard residual layers improves memory efficiency: . | . . Technique 2: Locality-Sensitive Hashing (LSH) based attention replaces dot-product attention (and is much faster) which reduces the time complexity: | . The 15 minute ICLR paper presentation video linked above really helps better understand these concepts | A PyTorch Reformer implementation can be found here | . &#9889; ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators &#9889; . ELECTRA brings a couple of novelties, resulting in a much more computationally efficient transformer to train. It is trained with: a Generator-Discriminator setup and | a new pre-training task called Replaced Token Detection | . | The Generator is trained to replace masked tokens (as per the standard MLM task), the Discriminator then tries to identify the token that has been replaced | . . One subtle thing to note is that if the generator happens to generate the correct token then that token is considered &quot;real&quot; instead of &quot;fake&quot; | ELECTRA-small can be trained on a single V100 GPU (4 days) | It is slower per epoch than other transformers, but it converges faster resulting in an overall faster training:the model learns from all input tokens instead of just the small masked-out subset, making it more computationally efficient . | Very strong results and it&#39;s performance scales up as the architecture is made larger | Lots more interesting results and experiment discussion can be found in the paper | A HuggingFace ELECTRA Implementation is here | . &#9889; Lite Transformer with Long-Short Range Attention &#9889; . Introduces Long-Short Range Attention (LRSA) which results in a reduction in model computation between 2.5x and 3x compared to original Transformer. . | The new architecture enables 2 different perspectives on the input sequence: . ...one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention)... . | The LSRA architecture and where the attention is focussed can be seen here: . | Lite Transformer performs well against the original Transformer for translation, summarisation and language modelling | One thing I liked is that Lite Transformer looks at performance under mobile-like constraints, defined by the authros as 10M parameters and 1G FLOPs | Lite Transformer code (PyTorch) is available from the authors here | . &#9889; ALBERT: A Lite BERT for Self-supervised Learning of Language Representations &#9889; . ALBERT is 18x smaller model than BERT-large and can be trained 1.7x faster while still outperforming it | The two techniques used to reduce its size are: Reduce the vocabulary embedding size; they reduce the matrix size by projecting it to a lower dimension. e.g. an input one-hot encoded matrix of size 30,000 is reduced to a much smaller sized matix which is then used | Cross-layer parameter sharing; they use the same operations and repeat them multiple times. This helps the parameter size of the network growing as layers as added | . | ALBERT uses 3 training tricks to further improve its performance: Uses MLM and Sentence Order Prediction (SOP), a self-supervised loss that focuses on modeling inter-sentence coherence | Does not use dropout (due to the huge amount of data available) | Uses 10x more data than BERT-Base | | HuggingFace PyTorch ALBERT code can be found here | . . Other Great Summaries to Read . Other great summaries from ICLR attendees are below, the Google Doc in Yacine&#39;s tweet below gives brief summaries to even more papers that I haven&#39;t covered here . Marija Stanojevic on mentorship tips for aspiring ML Researchers, @mstanojevic118 . | Yacine Jernite with additional paper summaries, @YRnite . | Analytics Vidhya with a summary of the event and what the most used opensource tools were . | . To Close . Research work on efficient NLP is moving rapidly and it was fascinating to see so many different approaches on display at ICLR this year, myself and my single GPU are super excited to see how fast things will develop this year 😆 . This was also the first ML conference I attended and found the (covid-caused) virtual format to work exceptionally well, my huge congrates to all of the organisers involved in pulling off a massive amount of work in such a short amount of time! . As always, I would love to hear if you have any comments, thoughts or criticisms at @mcgenergy .",
            "url": "https://www.ntentional.com/nlp/efficient-nlp/transformers/2020/05/05/iclr-hghlights.html",
            "relUrl": "/nlp/efficient-nlp/transformers/2020/05/05/iclr-hghlights.html",
            "date": " • May 5, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "FastHugs: Language Modelling with Tranformers and Fastai",
            "content": "This aims to be an end-to-end description with code of how to train a transformer language model using fastai (v2) and HuggingFace, enjoy! . TL;DR . Main interesting bits in this notebook: . Provides full code to train a transformer (RoBERTa) using a Masked Language Model task | Utilise&#39;s many of HuggingFace&#39;s tokenizer features within fastai | Make predictions of masked tokens like this: | . . Before we get started . First off, huge thanks as always to both the Fastai and HuggingFace teams for giving so much back to the community by open-sourcing so much | For an example of text sequence classification using HuggingFace and fastai, have a look at my previous notebook here . | This tutorial is heavily based on HuggingFace&#39;s &quot;How to train a new language model from scratch using Transformers and Tokenizers&quot; tutorial, I highly recommend checking that out too. I try and highlight throughout where code has been used, borrowed or inspired by HuggingFace&#39;s code. . | . MLM Tranform . I feel the most useful thing in this notebook is the MLMTokensLabels transform*. This carries out the Masked Language Model task that RoBERTa was originally trained on. . This will take tokens ids (tokens after the have been numericalized), select a subset and either mask a certain amount of them (for prediction) or replace them with other random token ids (for regularisation). This transform also creates our labels by copying the input token ids and masking the tokens that do not need to be predicted, so that no loss is calculated on them. . Note the if you wish to train BERT or other transformer language models you will probably need to use a different task, e.g. BERT was trained on 2 tasks simultaneously, MLM and Next Sentence Prediction (NSP). Have a look at any blog posts or arxiv paper of the transformer of interest to find which task was used to pretrain it. . *This transform code is a re-write of the mask_tokens function used in HugginFace&#39;s tutorial, code here . Pretraining + Fine-Tuning: . As shown in ULMFit, MultiFiT, and elsewhere, you will get better results on your downstream task if you first fine-tune your pretrained model with the text of the same domain as your pretrained task. e.g. training an IMDB movie review classifier who&#39;s language model was trained on wikipedia text. 1/ Really excited about this one! &quot;Don&#39;t Stop Pretraining: Adapt Language Models to Domains and Tasks&quot; is live! With @anmarasovic, @swabhz , @kylelostat , @i_beltagy , Doug Downey, and @nlpnoah, to appear at ACL2020. Paper: https://t.co/hVbSQYnclk Code: https://t.co/7wKgE1mUme . &mdash; Suchin Gururangan (@ssgrn) April 24, 2020 . Using a Custom Tokenizer? . This code has not been tested using a custom tokenizer. You may want to do so if your text is very specific to a certain domain. If so then you&#39;ll have to add a number of attributes to your tokenzier to be able to use the code here. I really recommend the HuggingFace language model tutorial linked above for an example of training your own tokenizer with your own dataset . Data . We&#39;ll use the IMDB_SAMPLE here, pretending we are fine-tuning our transformer model before doing sentiment classification on IMDB. If you are pretraining a language model from scratch you&#39;d aim to use a larger, more generic source like a wikipedia dataset. fastai have the full WikiText103 (100 million tokens) dataset available for easy download here if you&#39;d like to train an enligh language model from scratch: . path = untar_data(URLs.WIKITEXT) . HuggingFace Auto Classes . HuggingFace have a numer of useful &quot;Auto&quot; classes that enable you to create different models and tokenizers by changing just the model name. . AutoModelWithLMHead will define our Language model for us. This can either be a pretrained model or a randomly initialised model | AutoTokenizer will load our tokenizer and enable us grab our vocab | AutoConfig will define the model architecture and settings, note that we use the pretrained config here for ease of use, but one can easily modify this config if needed | model_name is the model architecture (and optionally model weights) you&#39;d like to use. Language Models tested so far with this notebook: roberta-base | You can find all of HuggingFace&#39;s models at https://huggingface.co/models, most, but not all of them are supported by AutoModel,AutoConfig and AutoTokenizer | . | . We can now easily call whichever transformer we like as below: . model_name = &#39;roberta-base&#39; lm_model_class = AutoModelWithLMHead config_dict = AutoConfig.from_pretrained(model_name) . HuggingFace Tokenizer &amp; Vocab . We use AutoTokenizer to generate our pretrained tokenizer. HuggingFace&#39;s get_vocab returns a token : index dict however Fastai expects vocab to be a list. Therefore we need to convert this dict to a list to be able to use it in fastai . #collapse tokenizer = AutoTokenizer.from_pretrained(model_name) tokenizer_vocab=tokenizer.get_vocab() tokenizer_vocab_ls = [k for k, v in sorted(tokenizer_vocab.items(), key=lambda item: item[1])] print(f&#39;Tokenizer &quot;{tokenizer.__class__}&quot; vocab length is : {len(tokenizer_vocab_ls)}&#39;) . . Tokenizer &#34;&lt;class &#39;transformers.tokenization_roberta.RobertaTokenizer&#39;&gt;&#34; vocab length is : 50265 . Special Tokens . Its always good to know what special tokens your tokenizer takes, lets have a look: . tokenizer.special_tokens_map . {&#39;bos_token&#39;: &#39;&lt;s&gt;&#39;, &#39;eos_token&#39;: &#39;&lt;/s&gt;&#39;, &#39;unk_token&#39;: &#39;&lt;unk&gt;&#39;, &#39;sep_token&#39;: &#39;&lt;/s&gt;&#39;, &#39;pad_token&#39;: &#39;&lt;pad&gt;&#39;, &#39;cls_token&#39;: &#39;&lt;s&gt;&#39;, &#39;mask_token&#39;: &#39;&lt;mask&gt;&#39;} . FastHugs Tokenizer . This tokenizer wrapper is initialised with the pretrained HF tokenizer, you can also specify the max_seq_len if you want longer/shorter sequences. Given text it returns tokens and adds separator tokens depending on the model type being used. . # collapse class FastHugsTokenizer(): &quot;&quot;&quot; transformer_tokenizer : takes the tokenizer that has been loaded from the tokenizer class model_name : model type set by the user max_seq_len : override default sequence length, typically 512 for bert-like models. `transformer_tokenizer.max_len_single_sentence` and `transformer_tokenizer.max_len_sentences_pair` both account for the need to add additional special tokens, i.e. for RoBERTa-base max_len_single_sentence==510, leaving space for the 2 additional special tokens to be added for the model&#39;s default 512 positional embeddings pair : whether a single sentence (sequence) or pair of sentences are used Returns: - Tokenized text, up to the max sequence length set by the user or the tokenzier default &quot;&quot;&quot; def __init__(self, transformer_tokenizer=None, model_name=&#39;roberta&#39;, max_seq_len=None, pretrained=True, pair=False, **kwargs): self.model_name, self.tok, self.max_seq_len=model_name, transformer_tokenizer, max_seq_len if pretrained: if self.max_seq_len: if pair: assert self.max_seq_len&lt;=self.tok.max_len_sentences_pair, &#39;WARNING: max_seq_len needs to be less than or equal to transformer_tokenizer.max_len_sentences_pair&#39; else: assert self.max_seq_len&lt;=self.tok.max_len_single_sentence, &#39;WARNING: max_seq_len needs to be less than or equal to transformer_tokenizer.max_len_single_sentence&#39; else: if pair: self.max_seq_len=ifnone(max_seq_len, self.tok.max_len_sentences_pair) else: self.max_seq_len=ifnone(max_seq_len, self.tok.max_len_single_sentence) def do_tokenize(self, o:str): &quot;&quot;&quot;Returns tokenized text, adds prefix space if needed, limits the maximum sequence length&quot;&quot;&quot; if &#39;roberta&#39; in model_name: tokens=self.tok.tokenize(o, add_prefix_space=True)[:self.max_seq_len] else: tokens = self.tok.tokenize(o)[:self.max_seq_len] return tokens def de_tokenize(self, o): &quot;&quot;&quot;Return string from tokens&quot;&quot;&quot; text=self.tok.convert_tokens_to_string(o) return text def __call__(self, items): for o in items: yield self.do_tokenize(o) . . The Fastai bit . fasthugstok and our tok_fn . Lets incorporate the tokenizer from HuggingFace into fastai-v2&#39;s framework by specifying a function called fasthugstok that we can then pass on to Tokenizer.from_df. (Note .from_df is the only method I have tested) . Max Seqence Length . max_seq_len is the longest sequece our tokenizer will output. We can also the max sequence length for the tokenizer by changing max_seq_len. It uses the tokenizer&#39;s default, typically 512. 1024 or even 2048 can also be used depending on your GPU memory. Note when using pretrained models you won&#39;t be able to use a max_seq_len larger than the default. . max_seq_len = None sentence_pair=False fasthugstok = partial(FastHugsTokenizer, transformer_tokenizer=tokenizer, model_name=model_name, max_seq_len=max_seq_len, sentence_pair=sentence_pair) . We create a MLMTokenizer class which inherits from fastai&#39;s Tokenizer in order to fully decode . #collapse class MLMTokenizer(Tokenizer): def __init__(self, tokenizer, rules=None, counter=None, lengths=None, mode=None, sep=&#39; &#39;, **kwargs): super().__init__(tokenizer, rules, counter, lengths, mode, sep) def _detokenize1(self, o):return self.tokenizer.de_tokenize(o) def decodes(self, o): return TitledStr(str(self._detokenize1(o))) . . Set up fastai&#39;s Tokenizer.from_df, we pass rules=[fix_html] to clean up some of HTML messiness in our text. If you do not want any rules then you sould pass rules=[] to override fastai&#39;s default text processing rules . #collapse fastai_tokenizer = MLMTokenizer.from_df(text_cols=&#39;text&#39;, res_col_name=&#39;text&#39;, tok_func=fasthugstok, rules=[fix_html], post_rules=[]) fastai_tokenizer.rules . . [&lt;function fastai2.text.core.fix_html(x)&gt;] . Add Special Tokens . BERT-like transformers require special tokens to be added to the sequence, depending on the task, so we need a transform for those too . class AddSpecialTokens(Transform): &quot;Add special token_ids to the numericalized tokens for Sequence Classification&quot; def __init__(self, tokenizer): self.tok=tokenizer def encodes(self, o): return(TensorText(self.tok.build_inputs_with_special_tokens(list(o)))) . Create MLM Dataset . #collapse class MLMTokensLabels(Transform): &#39;&#39;&#39; MLM task - Select subset of input token ids, given by `mlm_probability` - Mask a subset of these, `mask_token_prob` - Replace half of the first subset with random tokens - This code most comes from the `mask_tokens` function here https://github.com/huggingface/transformers/blob/a21d4fa410dc3b4c62f93aa0e6bbe4b75a101ee9/examples/run_language_modeling.py#L66 Returns: input ids and labels &#39;&#39;&#39; def __init__(self, tokenizer=None, mlm_probability=0.15, mask_token_prob=0.8): self.tok, self.mlm_probability, self.mask_token_prob=tokenizer, mlm_probability, mask_token_prob def _gen_probability_matrix(self, labels): # We sample a few tokens in each sequence for masked-LM training (with probability mlm_probability, defaults to 0.15 in Bert/RoBERTa) probability_matrix = torch.full(labels.shape, self.mlm_probability) special_tokens_mask = self.tok.get_special_tokens_mask(labels.tolist(), already_has_special_tokens=True) probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0) if self.tok._pad_token is not None: padding_mask = labels.eq(self.tok.pad_token_id) probability_matrix.masked_fill_(padding_mask, value=0.0) return probability_matrix def _replace_with_mask(self, inputs, labels, masked_indices): # for `mask_token_prob`% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK]) indices_replaced = torch.bernoulli(torch.full(labels.shape, self.mask_token_prob)).bool() &amp; masked_indices inputs[indices_replaced] = self.tok.convert_tokens_to_ids(self.tok.mask_token) return inputs, indices_replaced def _replace_with_other(self, inputs, labels, masked_indices, indices_replaced): # 1-`mask_token_prob`)/210% of the time, we replace masked input tokens with random word indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() &amp; masked_indices &amp; ~indices_replaced random_words = torch.randint(len(self.tok), labels.shape, dtype=torch.long) inputs[indices_random] = random_words[indices_random] return inputs def encodes(self, inputs): if self.tok.mask_token is None: raise ValueError(&quot;This tokenizer does not have a mask token which is necessary for masked language modeling.&quot;) labels = inputs.clone() # Get probability of whether a token will be masked probability_matrix = self._gen_probability_matrix(labels) # Create random mask indices according to probability matrix masked_indices = torch.bernoulli(probability_matrix).bool() # Mask the labels for indices that are NOT masked, we only compute loss on masked tokens labels[~masked_indices] = -100 # Randomly replace with mask token inputs, indices_replaced = self._replace_with_mask(inputs, labels, masked_indices) # Randomly replace with mask token inputs = self._replace_with_other(inputs, labels, masked_indices, indices_replaced) # The rest of the time (10% of the time) we keep the masked input tokens unchanged return (inputs,labels) . . # collapse @Numericalize def decodes(self,o): &#39;Add the ability to parse masks for the loss function, set as `-100`&#39; if isinstance(o, tuple): o=o[0] tmp_vocab=self.vocab.copy() tmp_vocab.append(&#39;&lt;loss_mask&gt;&#39;) o=[-1 if o_ == -100 else o_ for o_ in o] return L(tmp_vocab[o_] for o_ in o if tmp_vocab[o_] != PAD) . . # collapse @delegates(Datasets) class Datasets(Datasets): &quot;Doesn&#39;t create a tuple in __getitem__ as x is already a tuple&quot; def __init__(self, items=None, tfms=None, tls=None, n_inp=None, dl_type=None, **kwargs): super().__init__(items=items, tfms=tfms, tls=tls, n_inp=n_inp, dl_type=dl_type, **kwargs) def __getitem__(self, it): # same as Datasets.__getitem__ but not wrapped in a tuple res = [tl[it] for tl in self.tls] return res[0] if is_indexer(it) else list(zip(*res)) . . Our dataset is now ready to be created, lets look at an some of our (x,y) that will be passed to the model. When -100 is passed to our loss function (nn.CrossEntropyLoss) it will be ignored in the calculation. Our model will also ignore any padding tokens (usually defined as 1) when passed to it. . #collapse-hide splits = ColSplitter()(df) tfms=[attrgetter(&quot;text&quot;), fastai_tokenizer, Numericalize(vocab=tokenizer_vocab_ls), AddSpecialTokens(tokenizer), MLMTokensLabels(tokenizer)] dsets = Datasets(df, splits=splits, tfms=[tfms], dl_type=SortedDL) dsets[0][0][:20], dsets[0][1][:20] . . (tensor([ 0, 1890, 12, 5225, 24320, 12, 8494, 18421, 50264, 328, 14938, 1774, 630, 75, 190, 356, 69, 50264, 32819, 784]), tensor([ -100, -100, -100, 5225, -100, -100, -100, 18421, -100, -100, -100, -100, -100, 75, -100, -100, -100, 4505, -100, 784])) . Dataloader . Padding . We need to make sure our padding is done correctly as some transformer models prefer padding on the left while others prefer it on the right. tokenizer.padding_side will tell us which side is correct. e.g., BERT, Roberta prefers padding to the right, so we set pad_first=False . #collapse def pad_mlm_input(samples, pad_idx=1, pad_fields=[0,1], pad_first=False, max_seq_len=None, backwards=False): &quot;Function that collect `samples` and adds padding, modified `max_len_l` in fastai&#39;s `pad_input`&quot; pad_fields = L(pad_fields) #max_len_l = ifnone(max_seq_len, pad_fields.map(lambda f: max([len(s[f]) for s in samples]))) max_len_l = pad_fields.map(lambda f: max_seq_len) if backwards: pad_first = not pad_first def _f(field_idx, x): if isinstance(x, tuple): x=(x[0]) ## Added this line too, removes tuple if present if field_idx not in pad_fields: return x idx = pad_fields.items.index(field_idx) #TODO: remove items if L.index is fixed sl = slice(-len(x), sys.maxsize) if pad_first else slice(0, len(x)) pad = x.new_zeros(max_len_l[idx]-x.shape[0])+pad_idx x1 = torch.cat([pad, x] if pad_first else [x, pad]) if backwards: x1 = x1.flip(0) return retain_type(x1, x) return [tuple(map(lambda idxx: _f(*idxx), enumerate(s))) for s in samples] def transformer_mlm_padding(tokenizer=None, max_seq_len=None, sentence_pair=False): &#39;Uses `pad_fields=[0,1]` to pad both input and label&#39; if tokenizer.padding_side == &#39;right&#39;: pad_first=False else: pad_first=True max_seq_len = ifnone(max_seq_len, tokenizer.max_len) return partial(pad_mlm_input, pad_fields=[0,1], pad_first=pad_first, pad_idx=tokenizer.pad_token_id, max_seq_len=max_seq_len) . . #collapse padding=transformer_mlm_padding(tokenizer) bs=4 dls = dsets.dataloaders(bs=bs, before_batch=[padding]) . . Check our batch . We can see our special RoBERTa tokens (&#39;&lt;s&gt;&#39;, &#39;&lt;/s&gt;&#39;), which translate to 0, 2 in its vocab, have been added to the start and end of each sequence in the batch. Your can look at these indices in tokenizer.get_vocab() to confirm this. We can also see that most of the tokens in our target (text_) are masked out as we only want to calculate the loss on the ~15% of the text tokens that have been masked. . #collapse b=dls.one_batch() b[0].size(), b[1].size() . . (torch.Size([4, 512]), torch.Size([4, 512])) . #collapse dls.show_batch() . . text text_ . 0 &lt;s&gt; I&lt;mask&gt; fortunate enough to meet&lt;mask&gt; Pal segregatedand still have my DS:TMlishing&lt;mask&gt; autographed by&lt;mask&gt;&lt;mask&gt; at a convention shortly&lt;mask&gt; the release, and asked him why he chose to do the film &quot;camp&quot;. Before&lt;mask&gt; could answer, two studio flacks intercepted and lectured me on how the studio &quot;knew best&quot; and how &quot;no one will take such&lt;mask&gt; film seriously&quot;. I had been reading the Bantam reprints for&lt;mask&gt; couple of years thanks&lt;mask&gt; a&lt;mask&gt; (ComiCon attendees of the 1970s will recall 357hawk and his band? I was in&lt;mask&gt; couple&lt;mask&gt; years of that withnd), and had higher hopes than what we&lt;mask&gt;.&lt;mask&gt; nThe flacks insisted that no high adventure would ever be&lt;mask&gt; seriously, and so doing &#39;camp&lt;mask&gt; was the&lt;mask&gt; way. Several other fans jumped in gap my&lt;mask&gt;, with Pal listening as best he could. At the end of the little event, Pal&lt;mask&gt; up to&lt;mask&gt; and apologized,&lt;mask&gt; he could have done more and better. n nSTAR WARS put the lie to | &lt;loss_mask&gt;&lt;loss_mask&gt; was&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; George&lt;loss_mask&gt; (&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;OB poster&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; him)&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; after&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; he&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; &quot;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; a&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; a&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; to&lt;loss_mask&gt; friend&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Black&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; a&lt;loss_mask&gt; of&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; him&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; hopes&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; got&lt;loss_mask&gt; n&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; done&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&#39;&lt;loss_mask&gt;&lt;loss_mask&gt; only&lt;loss_mask&gt;.&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; on&lt;loss_mask&gt; side&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; came&lt;loss_mask&gt;&lt;loss_mask&gt; us&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; wishing&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;,&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&#39;s&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; that&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; it&lt;loss_mask&gt;&#39;t&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;.&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;,&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; rating as&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;.&lt;loss_mask&gt; n&lt;loss_mask&gt; destroying the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; still&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; have&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; to&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; we&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;hero&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;, there&lt;loss_mask&gt;&lt;loss_mask&gt; second&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&#39;s&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; serious&lt;loss_mask&gt; Yes&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; with&lt;loss_mask&gt;&lt;loss_mask&gt; And&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; a&lt;loss_mask&gt;&lt;loss_mask&gt;sheet&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; leaping&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; with&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; bronze&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; a&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; tie&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;AV&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Next&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; If&lt;loss_mask&gt; knows&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; George&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; San&lt;loss_mask&gt; for the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; | . 1 &lt;s&gt;&lt;mask&gt; is another one of those &#39;humans vs insects/eco-horror&#39; features; a theme that was popular in the late 70&#39;s.&lt;mask&gt; you can&#39;t really call it horror. There&#39;s zero suspense and no&lt;mask&gt; events.&lt;mask&gt; other words: this movie&lt;mask&gt; pretty lame. It&#39;s not that it&lt;mask&gt; really bad or&lt;mask&gt;; it&#39;s just very boring. A construction site near&lt;mask&gt; hotel uncovers a big nest of&lt;mask&gt;. Later on we learn that, probably due to&lt;mask&gt; sorts&lt;mask&gt; pesticides Lounge in the past, their&lt;mask&gt; became poisonous. Some people get bitten and rushed to&lt;mask&gt; hospital and it takes ages for&lt;mask&gt;&lt;mask&gt; Vanity the&lt;mask&gt; to figure out what&#39;s going on.&lt;mask&gt; Foxworth figures&lt;mask&gt; out first and then you can&lt;mask&gt; him go berserk with a digging machine for what seems like several hours.&lt;mask&gt; they&lt;mask&gt; in the house, waiting&lt;mask&gt; get rescued. And, man, you should see all the efforts they make for&lt;mask&gt; them.&lt;mask&gt; won&#39;t spoil too much, but at&lt;mask&gt; point they even use a big&lt;mask&gt;. All the | &lt;loss_mask&gt; This&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Only&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; gruesome&lt;loss_mask&gt;&lt;loss_mask&gt; In&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; is&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&#39;s&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; something&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; a&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; ants&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; different&lt;loss_mask&gt; of&lt;loss_mask&gt; used&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; bite&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the residents of&lt;loss_mask&gt; hospital&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Robert&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; it&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; see&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Then&lt;loss_mask&gt; flee&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; to&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; all&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; rescuing&lt;loss_mask&gt;&lt;loss_mask&gt; I&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; one&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; helicopter&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; this&lt;loss_mask&gt; I&lt;loss_mask&gt;&lt;loss_mask&gt; thinking&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; you&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; on&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; building&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; lots of&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; of&lt;loss_mask&gt;&lt;loss_mask&gt; are shown&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; movie&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Ant&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; garbage&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; in&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; straw&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; ants&lt;loss_mask&gt; wider shots&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; designers&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; near&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; do&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; in&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; It&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; as&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; IT&lt;loss_mask&gt;&lt;loss_mask&gt;EN&lt;loss_mask&gt; AT&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; my&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; title&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;K&lt;loss_mask&gt;&lt;loss_mask&gt; MAN&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; have&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;for&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&#39;ll&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; in&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Now&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;,&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; | . 2 &lt;s&gt; The&lt;mask&gt;&lt;mask&gt; saw no fewer than 3 filmed productions&lt;mask&gt; H. G. Wells&#39; great novel, &quot;War of&lt;mask&gt; Worlds&quot;. This&lt;mask&gt; perhaps the least well-known and very probably the best of&lt;mask&gt;&lt;mask&gt; No other&lt;mask&gt;&lt;mask&gt; W&lt;mask&gt;W has ever attempted not only to present the story very much as Wells wrote&lt;mask&gt;, but also Burton create the atmosphere of the time&lt;mask&gt; which it was supposed to take place: the last year of&lt;mask&gt; 19th Century, 1900  using Wells&#39; original setting, in and near Woking&lt;mask&gt;&lt;mask&gt;. n nIMDb&lt;mask&gt; unfFlyingly to what they regard as &quot;spoilers&quot;. That might apply&lt;mask&gt; some&lt;mask&gt;, where the ending might actually be a&lt;mask&gt;, but with regard to one of the most famous novels in&lt;mask&gt;&lt;mask&gt;, it seems positively silly. I have&lt;mask&gt; sympathy&lt;mask&gt; people who have neglected to&lt;mask&gt; one&lt;mask&gt; the seminal works&lt;mask&gt; English literature,&lt;mask&gt; let&#39;s get right to the chase. The aliens are destroyed through catching an Earth disease,&lt;mask&gt; hits&lt;mask&gt; have no immunity. If that wo a spoiler, so be | &lt;loss_mask&gt;&lt;loss_mask&gt; year 2005&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; of&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; is&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; them.&lt;loss_mask&gt;&lt;loss_mask&gt; version of&lt;loss_mask&gt;ot&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; it&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; to&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; in&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;, England&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; seems unfriend&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; with&lt;loss_mask&gt; films&lt;loss_mask&gt; where&lt;loss_mask&gt;&lt;loss_mask&gt; might&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; surprise&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the world&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; have no&lt;loss_mask&gt; for&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; read&lt;loss_mask&gt; of&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; in&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; so&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; against which they&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&#39;s&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; other&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; 1953 classic&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; to&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; n&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&#39; plot&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;, is&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; �&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; way&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; off due to&lt;loss_mask&gt;&lt;loss_mask&gt; of&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Century&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; in&lt;loss_mask&gt;&lt;loss_mask&gt;ides&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; film&lt;loss_mask&gt;&lt;loss_mask&gt; some&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; an&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; old&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; than&lt;loss_mask&gt;).&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; of&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;.&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; are typical of&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&#39;t&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;,&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; to&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;/white and&lt;loss_mask&gt; on&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;.&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; as&lt;loss_mask&gt; described them&lt;loss_mask&gt; have a more&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;feel&quot;.&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; destruction&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; more&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; period&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; particularly&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; or brilliant&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; facial&lt;loss_mask&gt; | . 3 &lt;s&gt;&lt;mask&gt; watched Grend&lt;mask&gt; the&lt;mask&gt; night and am compelled&lt;mask&gt; evangelical&lt;mask&gt; a Public Service Announcement. n nGrendel is another version of&lt;mask&gt;owulf, the thousand- resulted-&lt;mask&gt; Anglo-Saxon epic poem.&lt;mask&gt; SciFi channeluture a growing catalog of inoffensive&lt;mask&gt; uninterestingxs,&lt;mask&gt; the previews promised an&lt;mask&gt;authentic low-budget mini-epic, but this one refused to&lt;mask&gt;&lt;mask&gt; switch channels.&lt;mask&gt; was staggeringly, overwhelmingly, bad&lt;mask&gt; I watched in fascination and horror at the train wreck you&lt;mask&gt;&#39;t tear your eyes away from&lt;mask&gt; I reached for a notepad and managed to capture part of what I was seeing.&lt;mask&gt; following may contain spoilers or might just save your sanity&lt;mask&gt; You&#39;ve been warned. n n- Just to&lt;mask&gt; it over with, Beow&lt;mask&gt;&lt;mask&gt; warriors wore horned&lt;mask&gt;.&lt;mask&gt;&lt;mask&gt;ial issue compared to what came after. It also appears that the helmets were in a bin and handed&lt;mask&gt; whichever actor wandered by next. Fit,&lt;mask&gt; and function&lt;mask&gt; apparently irrelevant. n n- Marina Sirtis&lt;mask&gt;&lt;mask&gt; been blackmailed into doing the&lt;mask&gt; by&lt;mask&gt; Ringling Brothers, Barnum and&lt;mask&gt;&lt;mask&gt;.&lt;mask&gt; managed to avoid a red rubber nose, but the | &lt;loss_mask&gt; I&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;el&lt;loss_mask&gt; other&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; to put together&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Be&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;year&lt;loss_mask&gt;old&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; The&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; has&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; and&lt;loss_mask&gt;&lt;loss_mask&gt; movies&lt;loss_mask&gt; and&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; in&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; let me&lt;loss_mask&gt;&lt;loss_mask&gt;. It&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;.&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; couldn&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;.&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; The&lt;loss_mask&gt;&lt;loss_mask&gt; contain&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; save&lt;loss_mask&gt;&lt;loss_mask&gt;.&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; n&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; get&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;ulf&#39;s&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; helmets&lt;loss_mask&gt; Triv&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; to&lt;loss_mask&gt; actor&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; appearance&lt;loss_mask&gt;&lt;loss_mask&gt; were&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; had obviously&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; movie&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Bailey circus&lt;loss_mask&gt; She&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; Ben&lt;loss_mask&gt;&lt;loss_mask&gt; not&lt;loss_mask&gt; be&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; H&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; must have&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; film&lt;loss_mask&gt;&lt;loss_mask&gt; hadn&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; to&lt;loss_mask&gt; him&lt;loss_mask&gt;&lt;loss_mask&gt; n&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; facilitate&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; hairst&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;.&lt;loss_mask&gt;&lt;loss_mask&gt; of&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; sideburn&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; and&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;.&lt;loss_mask&gt; prove&lt;loss_mask&gt;&lt;loss_mask&gt; a&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;-&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; movie&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; this&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;shaped&lt;loss_mask&gt;&lt;loss_mask&gt;-&lt;loss_mask&gt; and&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; tradition&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; with&lt;loss_mask&gt; volume&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; n&lt;loss_mask&gt;&lt;loss_mask&gt; unintended focus&lt;loss_mask&gt;&lt;loss_mask&gt; movie&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; with&lt;loss_mask&gt; bolts&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; recoil&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; the&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt;&lt;loss_mask&gt; | . Model . Our model can be instantiated with either pretrained or random weights. We also need to be careful to pass the model the attention_mask so that the model ignores padding tokens when training. . class LMModel(nn.Module): def __init__(self, lm_model_class=None, tokenizer=None, model_name=None, config_dict=None, pretrained=False): super().__init__() self.tok=tokenizer if pretrained: self.model = lm_model_class.from_pretrained(model_name) else: self.model = lm_model_class.from_config(config_dict) self.model = self.model.module if hasattr(self.model, &quot;module&quot;) else self.model self.model.resize_token_embeddings(len(tokenizer)) def forward(self, input_ids): attention_mask = (input_ids!=self.tok.pad_token_id).type(input_ids.type()) return self.model(input_ids, attention_mask=attention_mask)[0] # only return the prediction_scores (and not hidden states and attention) . Pretrained Language Model . Lets fine-tune our pretrained Language Model. We would typically do this before training the model on our specific text. Note that here we are not training the language model head before we train the full model, but we could do so if we created a splitter and passed it to our learner . To load the pretrained HuggingFace model just use pretrained=True when calling your model: . model = LMModel(lm_model_class=lm_model_class, tokenizer=tokenizer, model_name=model_name, config_dict=config_dict, pretrained=True) . Training . From here we train our model as usual using fastai. Note that we use Perplexity as our metric as it is a good measure of how well a language model is training . #collapse opt_func = partial(Adam, decouple_wd=True) loss = CrossEntropyLossFlat() learn = Learner(dls, model, opt_func=opt_func, #splitter=model_splitter, loss_func=loss, metrics=[accuracy, Perplexity()]).to_fp16() . . We check our learning rate finder . #collapse-hide learn.lr_find(suggestions=True, stop_div=False) . . SuggestedLRs(lr_min=0.025118863582611083, lr_steep=0.2089296132326126) . We do some training . #collapse-hide learn.fit_one_cycle(10, lr_max=1e-4) . . epoch train_loss valid_loss accuracy perplexity time . 0 | 13.052832 | 11.317341 | 0.052725 | 82235.375000 | 00:32 | . 1 | 8.539399 | 5.935386 | 0.049121 | 378.185822 | 00:32 | . 2 | 1.660586 | 0.785814 | 0.493350 | 2.194192 | 00:32 | . 3 | 0.731211 | 0.768679 | 0.493125 | 2.156916 | 00:32 | . 4 | 0.732979 | 0.772890 | 0.492373 | 2.166016 | 00:32 | . 5 | 0.681202 | 0.695503 | 0.493711 | 2.004716 | 00:33 | . 6 | 0.660206 | 0.681334 | 0.494063 | 1.976512 | 00:33 | . 7 | 0.469388 | 0.641964 | 0.495615 | 1.900209 | 00:33 | . 8 | 0.512519 | 0.612524 | 0.494834 | 1.845082 | 00:33 | . 9 | 0.545736 | 0.625833 | 0.495205 | 1.869804 | 00:33 | . And we see how our loss progressed . Lets Look at the model&#39;s predictions . Manually checking how well our model makes predictions for masked tokens is a simple way to see how it is training . Here function get_mask_pred takes masked string given by the user and returns the topk predictions given by the model for that masked token. With it we can sanity check that our model has learned something useful! . *Note that get_mask_pred is mostly code from FillMaskPipeline in HuggingFace&#39;s Transformers repo, full credit to them! . #collapse def get_mask_pred(model, masked_text:str, topk:int=5): &quot;Code lightly modified from `FillMaskPipeline` in the HuggingFace Transformers library&quot; aa=fastai_tokenizer.encodes(masked_text) bb=Numericalize(vocab=tokenizer_vocab_ls)(aa) cc=AddSpecialTokens(tokenizer)(bb) outs=model(cc.unsqueeze(0).cuda()) masked_index = (cc == tokenizer.mask_token_id).nonzero().item() logits = outs[0, masked_index, :] probs = logits.softmax(dim=0) values, predictions = probs.topk(topk) result=[] for i, vv in enumerate(zip(values.tolist(), predictions.tolist())): v, p =vv tokens = cc.numpy() if i == 0: result.append({&quot;word&quot;:&quot;Input text&quot;, &quot;score&quot;: 0., &quot;token&quot;: 0, &quot;sequence&quot;: tokenizer.decode(tokens)}) tokens[masked_index] = p tokens = tokens[np.where(tokens != tokenizer.pad_token_id)] w = tokenizer.decode(p) result.append({&quot;word&quot;:w, &quot;score&quot;: v, &quot;token&quot;: p, &quot;sequence&quot;: tokenizer.decode(tokens)}) return pd.DataFrame(result) . . Here we can input our own masked sentence and see how the model does. Note that even without fine-tuning the performance below will still be very strong as the pretrained RoBERTa model is very strong. . text2 = &#39;I was walking to &lt;mask&gt; when I came across a cat on the road&#39; pred2 = get_mask_pred(model1, text2);pred2.head() . word score token sequence . 0 Input text | 0.000000 | 0 | &lt;s&gt; I was walking to&lt;mask&gt; when I came across a cat on the road&lt;/s&gt; | . 1 school | 0.791473 | 334 | &lt;s&gt; I was walking to school when I came across a cat on the road&lt;/s&gt; | . 2 church | 0.068957 | 2352 | &lt;s&gt; I was walking to church when I came across a cat on the road&lt;/s&gt; | . 3 work | 0.068007 | 173 | &lt;s&gt; I was walking to work when I came across a cat on the road&lt;/s&gt; | . 4 breakfast | 0.007202 | 7080 | &lt;s&gt; I was walking to breakfast when I came across a cat on the road&lt;/s&gt; | . Not bad at all! Now lets see how it does on a movie review, lets look at an example from our validation set. We mask the word might from the first sentence of the reivew, ... shows what might happen... . mask_indices=[7] txts=df.text.values masked_text = txts[800].split(&#39; &#39;) # our validation split starts at index 800 masked_text[mask_indices[0]] = &#39;&lt;mask&gt;&#39; masked_text = &quot; &quot;.join(masked_text) pred1 = get_mask_pred(model1, masked_text);pred1.head() . word score token sequence . 0 Input text | 0.000000 | 0 | &lt;s&gt; This very funny British comedy shows what&lt;mask&gt; happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;b... | . 1 would | 0.809723 | 74 | &lt;s&gt; This very funny British comedy shows what would happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;b... | . 2 might | 0.131539 | 429 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;b... | . 3 could | 0.042638 | 115 | &lt;s&gt; This very funny British comedy shows what could happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;b... | . 4 will | 0.009556 | 40 | &lt;s&gt; This very funny British comedy shows what will happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br... | . Boom, pretty darn good! Lets try the same example, replacing ancient in discovery of ancient documents . mask_indices=[54] txts=df.text.values masked_text = txts[800].split(&#39; &#39;) # our validation split starts at index 800 masked_text[mask_indices[0]] = &#39;&lt;mask&gt;&#39; masked_text = &quot; &quot;.join(masked_text) pred1 = get_mask_pred(model, masked_text);pred1.head() . word score token sequence . 0 Input text | 0.000000 | 0 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of&lt;mask&gt; documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br ... | . 1 historical | 0.585666 | 4566 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of historical documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /... | . 2 old | 0.086817 | 793 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of old documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;... | . 3 obscure | 0.040825 | 23732 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of obscure documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;b... | . 4 ancient | 0.035504 | 8178 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;b... | . Again, pretty solid predictions! . Train a Language Model from Scratch! . We can follow the same procedure to train a language model from scratch by using pretrained=False when seeing up our model . #collapse model = LMModel(lm_model_class=lm_model_class, tokenizer=tokenizer, model_name=model_name, config_dict=config_dict, pretrained=False) opt_func = partial(Adam, decouple_wd=True) loss = CrossEntropyLossFlat() learn = Learner(dls, model, opt_func=opt_func, loss_func=loss, metrics=[accuracy, Perplexity()]).to_fp16() . . Training . Untrained . Again, lets look at the predictions: . model2=learn.model . text2 = &#39;I was walking to &lt;mask&gt; when I cam across a cat on the road&#39; pred2 = get_mask_pred(model2, text2);pred2.head() . word score token sequence . 0 Input text | 0.000000 | 0 | &lt;s&gt; I was walking to&lt;mask&gt; when I cam across a cat on the road&lt;/s&gt; | . 1 the | 0.037963 | 5 | &lt;s&gt; I was walking to the when I cam across a cat on the road&lt;/s&gt; | . 2 . | 0.036504 | 4 | &lt;s&gt; I was walking to. when I cam across a cat on the road&lt;/s&gt; | . 3 , | 0.033266 | 6 | &lt;s&gt; I was walking to, when I cam across a cat on the road&lt;/s&gt; | . 4 of | 0.024381 | 9 | &lt;s&gt; I was walking to of when I cam across a cat on the road&lt;/s&gt; | . Pretty bad 👎, and see how the unconfident it is in its predictions! This doesn&#39;t perform well because we have only used 800 movie reviews to train our model, we&#39;ll need a lot more text to get decent results! . Again, just for fun, lets see how it does on a movie review, lets look at an example from our validation set. We mask the word might from the first sentence of the reivew, ... shows what might happen... . mask_indices=[7] txts=df.text.values masked_text = txts[800].split(&#39; &#39;) # our validation split starts at index 800 masked_text[mask_indices[0]] = &#39;&lt;mask&gt;&#39; masked_text = &quot; &quot;.join(masked_text) pred1 = get_mask_pred(model2, masked_text);pred1.head() . word score token sequence . 0 Input text | 0.000000 | 0 | &lt;s&gt; This very funny British comedy shows what&lt;mask&gt; happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;b... | . 1 , | 0.044226 | 6 | &lt;s&gt; This very funny British comedy shows what, happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;S... | . 2 the | 0.035027 | 5 | &lt;s&gt; This very funny British comedy shows what the happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br ... | . 3 . | 0.028172 | 4 | &lt;s&gt; This very funny British comedy shows what. happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;S... | . 4 and | 0.025764 | 8 | &lt;s&gt; This very funny British comedy shows what and happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br ... | . Ewww.. . mask_indices=[54] txts=df.text.values masked_text = txts[800].split(&#39; &#39;) # our validation split starts at index 800 masked_text[mask_indices[0]] = &#39;&lt;mask&gt;&#39; masked_text = &quot; &quot;.join(masked_text) pred1 = get_mask_pred(model2, masked_text);pred1.head() . word score token sequence . 0 Input text | 0.000000 | 0 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of&lt;mask&gt; documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br ... | . 1 the | 0.036510 | 5 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of the documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;... | . 2 , | 0.035627 | 6 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of, documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;Sta... | . 3 and | 0.029176 | 8 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of and documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;... | . 4 . | 0.029063 | 4 | &lt;s&gt; This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of. documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;Sta... | . Yuck! . Notes &amp; Hacky Bits . Notes . The validation set will change slightly due to random masking. While the data in the validaion set remains constant, different tokens will be masked each time the validation dataloader is called due to MLMTokensLabels calling a random probability each time. . If a perfectly reproducable validation set is needed then you&#39;ll probably have to create a separate transform for it&#39;s masking and set it&#39;s split_idx = 1. | . | I didn&#39;t have time to get learn.predict working. One issue that needs to be fixed is that MLMTokensLabels transform shouldn&#39;t be called on your masked input text as it will add more masks, which you don&#39;t want. . | FastHugsTokenizer will have to be modified to: . enable sequence lengths larger than the tokenizer default | to use a non-pretrained tokenizer (e.g. one you trained yourself) | . | The HuggingFace encode_plus or batch_encode_plus functions are great and I would have used them, but don&#39;t play nice with fastai multiprocessiing . | . Hacks . I had to overwrite __getitem__ in the Datasets class so that it wouldn&#39;t return a tuple as what it thinks is our x is actually our (x,y). Wrapping this tuple in anoother tuple causes headaches down the line. Creating a custom Datasets class and inheriting from it didn&#39;t work as learn.predict calls on Datasets and not the custom dataset class. . | The function get_mask_pred (used to view predictions of masked text) is mostly code from FillMaskPipeline in HuggingFace&#39;s Transformers repo, full credit to them! . | . Give me a shout &#128227; . Thats it for this, I hope you found it useful and learned a thing or two. If you have any questions or would like to get in touch you can find me on Twitter @mcgenergy .",
            "url": "https://www.ntentional.com/nlp/transformers/training%20technique/classification/2020/04/24/fasthugs_language_model.html",
            "relUrl": "/nlp/transformers/training%20technique/classification/2020/04/24/fasthugs_language_model.html",
            "date": " • Apr 24, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://www.ntentional.com/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://www.ntentional.com/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Morgan (Me)",
          "content": "Current on a self-directed learning journey, diving deep into ML and loving it . Previously at Facebook and before then working in electricity trading at Gaelectric and Danske Commodities . I really enjoy ML talk, give me a shout on @mcgenergy on Twitter or on LinkedIn or have a look at my latest work (such as HuggingFace, a bridge between between fastai and the HuggingFace library) on my Github . .",
          "url": "https://www.ntentional.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "",
          "content": ". Welcome! My latest articles are below, if you’d like to get in touch, find me at @mcgenergy on Twitter . Latest Articles 👇 .",
          "url": "https://www.ntentional.com/",
          "relUrl": "/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://www.ntentional.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}